{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import pandas as pd\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"master_data\" : r\"C:\\Users\\nigam\\OneDrive\\Documents\\university_classes\\AutoMMM\\data\\manual_data.xlsx\",\n",
    "    \"sheet_name\" : \"master_data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message,tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_config(config: dict):\n",
    "    master_data = pd.read_excel(config['master_data'],sheet_name=config['sheet_name'])\n",
    "\n",
    "    return {\n",
    "        'master_data' : master_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration = process_config(config)\n",
    "\n",
    "# configuration['master_data'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"mistral-saba-24b\", # llama3-70b-8192, llama-3.3-70b-versatile\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! How can I assist you today? If you're up for it, let's play a game of 20 questions. You think of something, and I'll try to guess it by asking up to 20 yes-or-no questions. Sound good?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 6, 'total_tokens': 61, 'completion_time': 0.166666667, 'prompt_time': 0.001975514, 'queue_time': 0.088087122, 'total_time': 0.168642181}, 'model_name': 'mistral-saba-24b', 'system_fingerprint': 'fp_07e680a590', 'finish_reason': 'stop', 'logprobs': None}, id='run--6a351d68-0736-4e67-aeef-e052d3b14981-0', usage_metadata={'input_tokens': 6, 'output_tokens': 55, 'total_tokens': 61})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ContentAnalyzerAgent: Analyzing text with LLM... ---\n",
      "Content analysis complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import pandas as pd\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, List,  Dict, Optional, Union\n",
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown, Image\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import subprocess\n",
    "\n",
    "\n",
    "llm_infograph = llm\n",
    "\n",
    "class DataPoint(BaseModel):\n",
    "    type: str \n",
    "    description: str\n",
    "    value: Union[str, int, float, List[Union[str, int, float]], dict]\n",
    "    context_text: Optional[str] = None\n",
    "\n",
    "class Section(BaseModel):\n",
    "    heading: str\n",
    "    intro_text: str\n",
    "    data_points: List[DataPoint]\n",
    "\n",
    "class InfographicData(BaseModel):\n",
    "    infographic_title: str\n",
    "    sections: List[Section]\n",
    "\n",
    "\n",
    "content_analyzer_llm = llm_infograph.with_structured_output(InfographicData)\n",
    "\n",
    "print(\"--- ContentAnalyzerAgent: Analyzing text with LLM... ---\")\n",
    "raw_text = \"\"\"\n",
    "# LLM Response\n",
    "\n",
    "## Market Mix Modeling Data Analysis Report\n",
    "\n",
    "This report analyzes the provided market mix data to understand the factors influencing sales for various products.\n",
    "\n",
    "### 1. Unique Products\n",
    "\n",
    "The dataset includes sales data for at least three unique products: `sku_a`, `sku_b`, and `sku_c`.\n",
    "\n",
    "### 2. Data Shape\n",
    "\n",
    "The data is structured as a time series, with each row representing a specific date and product combination.\n",
    "\n",
    "### 3. Column Breakdown\n",
    "\n",
    "The data comprises 19 columns, which can be categorized as follows:\n",
    "\n",
    "#### 3.1. Base Data\n",
    "\n",
    "* **date:** Date of the sales record (YYYY-MM-DD format).\n",
    "* **sku:** Unique identifier for each product.\n",
    "* **sales:** Total sales revenue for the product on that date.\n",
    "* **units:** Number of units sold for the product on that date.\n",
    "* **price:** Average selling price per unit for the product on that date.\n",
    "\n",
    "#### 3.2. Incremental Marketing Spend\n",
    "\n",
    "* **online_spends:** Marketing spend allocated to online channels for the product on that date.\n",
    "* **offline_spends:** Marketing spend allocated to offline channels for the product on that date.\n",
    "\n",
    "#### 3.3. External Features\n",
    "\n",
    "* **competitor_spends:** Estimated marketing spend by competitors for the product category on that date.\n",
    "* **seasonality_index:** A measure of seasonal demand for the product category on that date (e.g., higher during holidays).\n",
    "* **economic_indicator:** A relevant economic indicator (e.g., GDP growth, unemployment rate) that may influence sales.\n",
    "\n",
    "#### 3.4. Social Media Marketing\n",
    "\n",
    "* **insta_clicks:** Number of clicks on Instagram ads for the product on that date.\n",
    "* **insta_spends:** Marketing spend allocated to Instagram ads for the product on that date.\n",
    "* **fb_clicks:** Number of clicks on Facebook ads for the product on that date.\n",
    "* **fb_spends:** Marketing spend allocated to Facebook ads for the product on that date.\n",
    "\n",
    "#### 3.5. Other Features\n",
    "\n",
    "* **brand_level_nonbranded_spends:** Marketing spend allocated to non-branded campaigns for the brand on that date.\n",
    "* **promotions:** A binary variable indicating whether a promotion was active for the product on that date.\n",
    "\n",
    "### 4. Business Implications\n",
    "\n",
    "This data allows us to:\n",
    "\n",
    "* **Measure the effectiveness of different marketing channels:** By analyzing the relationship between marketing spend (online, offline, social media) and sales, we can identify which channels are most effective for driving sales.\n",
    "* **Understand the impact of external factors:** Analyzing the relationship between external factors (competitor spend, seasonality, economic indicators) and sales can help us anticipate market trends and adjust our strategies accordingly.\n",
    "* **Optimize marketing budget allocation:** By understanding the return on investment (ROI) for different marketing activities, we can allocate our budget more effectively to maximize sales.\n",
    "* **Identify opportunities for growth:** Analyzing the data can reveal untapped market segments or opportunities for product development.\n",
    "\n",
    "\n",
    "\n",
    "This report provides a high-level overview of the data. Further analysis and modeling are required to draw more specific conclusions and actionable insights.\n",
    "\n",
    "---\n",
    "\n",
    "## Market Mix Modelling Report: SKU \"sku_a\"\n",
    "\n",
    "This report analyzes the sales data for SKU \"sku_a\" from 2025-06-07 to 2027-05-29, focusing on identifying patterns, outliers, missing data, and correlations between key performance indicators (KPIs).\n",
    "\n",
    "### 1. Sales Pattern Throughout the Timeline\n",
    "\n",
    "* **General Trend:** Sales for SKU \"sku_a\" exhibit a fluctuating pattern over the analyzed period. \n",
    "* **Seasonality:** While a clear seasonal trend isn't immediately apparent, there are noticeable peaks and valleys in sales throughout the two-year period. Further analysis, potentially incorporating time-based features, could reveal underlying seasonal patterns.\n",
    "\n",
    "### 2. Date Distribution and Spread\n",
    "\n",
    "* **Even Distribution:** The data appears to be relatively evenly distributed across the available dates, with sales recorded for most days within the timeframe.\n",
    "\n",
    "### 3. Outliers\n",
    "\n",
    "* **Identification:**  There are no significant outliers in the sales data for SKU \"sku_a\" based on a visual inspection. \n",
    "* **Possible Reasons:**  The absence of outliers could indicate consistent marketing efforts and stable market conditions.\n",
    "* **Impact on Sales:**  The lack of outliers suggests that the sales data is relatively stable and reliable.\n",
    "* **Suggestion:**  Continue monitoring sales data for any unexpected spikes or drops that might indicate emerging outliers.\n",
    "\n",
    "### 4. Missing Data\n",
    "\n",
    "* **Percentage:** There are no significant instances of missing data in the provided dataset.\n",
    "* **Pattern:** The data appears to be complete, with no noticeable patches or irregular gaps.\n",
    "* **Possible Reasons:**  The absence of missing data suggests robust data collection practices.\n",
    "* **Impact on Sales:**  The complete dataset allows for accurate analysis and modeling of sales trends.\n",
    "* **Suggestion:**  Maintain consistent data collection procedures to ensure data integrity.\n",
    "\n",
    "### 5. Count of Zeros\n",
    "\n",
    "* **Occurrence:**  There are a few instances of zero sales recorded for SKU \"sku_a\".\n",
    "* **Pattern:** These zero sales occurrences appear sporadic and not clustered in specific periods.\n",
    "* **Percentage:** The percentage of zero sales is relatively low, indicating that sales activity is generally present.\n",
    "* **Suggestion:** Investigate the reasons behind these zero sales instances. It could be due to temporary stock unavailability, promotional periods, or other market factors.\n",
    "\n",
    "### 6. Distinct Values (Categorical Columns)\n",
    "\n",
    "*  Please provide the categorical columns in the dataset for a detailed analysis of distinct values.\n",
    "\n",
    "### 7. Impact of Each Column on Sales\n",
    "\n",
    "*  A comprehensive analysis of the impact of each column on sales requires statistical modeling techniques such as regression analysis. This will allow us to quantify the contribution of each KPI to sales performance.\n",
    "\n",
    "### 8. Inter-KPI Correlations\n",
    "\n",
    "*  Correlation analysis will be conducted to identify relationships between different KPIs. This will help understand how marketing spend, social media activity, and other factors influence sales.\n",
    "\n",
    "### 9. Heatmap for Correlation\n",
    "\n",
    "*  A heatmap will be generated to visualize the correlation matrix of all KPIs. This will provide a clear and intuitive representation of the relationships between the variables.\n",
    "\n",
    "\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "* Provide the categorical columns for analysis.\n",
    "* Conduct regression analysis to quantify the impact of each KPI on sales.\n",
    "* Generate a correlation matrix and heatmap to visualize inter-KPI relationships.\n",
    "\n",
    "\n",
    "\n",
    "This report provides a preliminary overview of the sales data for SKU \"sku_a\". Further analysis and modeling will provide deeper insights into the factors driving sales performance and inform strategic decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## Market Mix Modelling Report: Product - sku_c\n",
    "\n",
    "This report analyzes the provided data for product sku_c, focusing on sales patterns, data quality, and key performance indicator (KPI) relationships.\n",
    "\n",
    "### 1. Sales Pattern Throughout the Timeline:\n",
    "\n",
    "The sales data for sku_c shows a general upward trend over the observed period.  \n",
    "\n",
    "* **Peak Sales:** Sales appear to peak around mid-June and mid-May of the respective years.\n",
    "* **Seasonal Fluctuations:** There are noticeable seasonal fluctuations, with sales potentially higher in the summer months.\n",
    "\n",
    "### 2. Date Distribution and Spread\n",
    "\n",
    "The data spans from June 2025 to May 2027, with a relatively even distribution of data points across the timeframe.\n",
    "\n",
    "### 3. Outliers\n",
    "\n",
    "* **No significant outliers** were identified in the sales data.\n",
    "\n",
    "### 4. Missing Data\n",
    "\n",
    "* **Percentage of Missing Points:** The data appears to have minimal missing points. A precise percentage will require a thorough scan of the dataset.\n",
    "* **Pattern of Missing Points:** No clear pattern of missing data was observed. \n",
    "* **Possible Reasons for Missing Data:** Potential reasons for any missing data could include:\n",
    "    * Data entry errors\n",
    "    * System glitches\n",
    "    |\n",
    "    * Temporary unavailability of data sources\n",
    "* **Impact on Sales Analysis:** A small amount of missing data is unlikely to significantly impact the overall sales analysis.\n",
    "* **Suggestion to Treat Missing Data:** If missing data points are identified, imputation techniques (e.g., mean/median imputation) could be used to fill them in.\n",
    "\n",
    "### 5. Count of Zeros in the Data\n",
    "\n",
    "* **Number of Zeros:** The number of zero sales values will need to be counted and analyzed.\n",
    "* **Pattern of Zeros:** It's important to determine if zero sales occur in clusters or are randomly distributed.\n",
    "\n",
    "### 6. Distinct Values (Categorical Columns)\n",
    "\n",
    "* **List distinct values** for each categorical column (e.g., brand, product category) and their frequencies.\n",
    "\n",
    "### 7. Impact of Each Column on Sales\n",
    "\n",
    "* **Regression Analysis:** Conduct a regression analysis to quantify the impact of each KPI on sales. This will reveal which factors have the strongest influence on sales performance.\n",
    "\n",
    "### 8. Inter-KPI Correlations\n",
    "\n",
    "* **Correlation Matrix:** Calculate the correlation matrix for all KPIs. This will highlight relationships between different variables.\n",
    "\n",
    "### 9. Heatmap for Correlation\n",
    "\n",
    "* **Visualize Correlations:** Create a heatmap to visually represent the correlation matrix. This will make it easier to identify strong positive and negative correlations.\n",
    "\n",
    "\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "* Complete the analysis of missing data, zero sales, and distinct values.\n",
    "* Conduct regression analysis and generate a correlation matrix.\n",
    "* Create a heatmap to visualize the correlation matrix.\n",
    "* Based on the findings, develop actionable insights and recommendations for optimizing marketing spend and driving sales growth for sku_c.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for ContentAnalyzerAgent\n",
    "content_analyzer_prompt = f\"\"\"\n",
    "You are an expert Data Analyst and Content Summarizer. Your task is to meticulously read the provided text and extract all relevant information, structuring it into a precise JSON format for subsequent visualization. Do not interpret or design any visuals; focus solely on accurate content analysis and structuring.\n",
    "\n",
    "**Input Text:**\n",
    "{raw_text}\n",
    "\n",
    "**Output Format (JSON Schema):**\n",
    "```json\n",
    "{{\n",
    "    \"infographic_title\": \"string (concise, catchy title based on main topic, max 8 words)\",\n",
    "    \"sections\": [\n",
    "    {{\n",
    "        \"heading\": \"string (main heading for this section, max 10 words)\",\n",
    "        \"intro_text\": \"string (brief introductory paragraph for this section, max 100 words)\",\n",
    "        \"data_points\": [\n",
    "        {{\n",
    "            \"type\": \"string (e.g., 'KPI', 'Trend', 'Comparison', 'ProcessStep', 'Definition', 'TimelineEvent', 'StatisticalSummary', 'KeyInsight')\",\n",
    "            \"description\": \"string (brief explanation of what this data point represents, max 20 words)\",\n",
    "            \"value\": \"any (numeric value, array of strings/numbers, object, or string depending on type. For processes, use an array of strings for steps.)\",\n",
    "            \"context_text\": \"string (short snippet of original text for context, max 50 words, optional)\"\n",
    "        }}\n",
    "        ]\n",
    "    }}\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "**Instructions:**\n",
    "1. Read the entire text carefully to grasp the overall context.\n",
    "2. Identify distinct logical sections within the text. For each section, create a 'heading' and a brief 'intro_text'.\n",
    "3. Within each section, identify all potential data points suitable for visualization. Extract them accurately.\n",
    "4. For each 'data_point':\n",
    "    * Assign the most appropriate 'type' from the suggested list.\n",
    "    * Write a concise 'description'.\n",
    "    * Extract its 'value'. Ensure numeric values are actual numbers, arrays are actual arrays, etc.\n",
    "    * Provide 'context_text' if a short direct quote or reference from the original text helps.\n",
    "5. Ensure the entire output is a single, valid, complete JSON object. Do NOT include any explanations or conversational text outside the JSON.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    extracted_data = content_analyzer_llm.invoke([{\"role\": \"user\", \"content\": content_analyzer_prompt}])\n",
    "    print(\"Content analysis complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in ContentAnalyzerAgent: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nigam\\AppData\\Local\\Temp\\ipykernel_21116\\1979893817.py:3: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  ouptut = json.loads(extracted_data.json())\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output = json.loads(extracted_data.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = df.profile_report(\n",
    "    variables={\n",
    "        \"descriptions\": {\n",
    "            \"date\" : \"Date column\",\n",
    "            \"sku\"\t: \"product identification number (stock keeping unit)\",\n",
    "            \"sales\" : \"amount of units sold (revenue in euros)\",\n",
    "            \"units\" : \"Number of units sold\",\n",
    "            \"price\" : \"average price of the product in the week\",\n",
    "            \"oos\"\t: \"number of days the product was out of stock in a week\",\n",
    "            \"events\" : \"number of events in a week\",\n",
    "            \"product_level_branded_clicks\" : \"clicks generated by product based search in a week (Branded keywords) \",\n",
    "            \"product_level_branded_spends\" : \"amount of money spent on clicks generated by product based search in the week (Branded keywords)\",\n",
    "            \"product_level_nonbranded_clicks\" : \"clicks generated by product based search in the week (Non - Branded keywords) \",\n",
    "            \"product_level_nonbranded_spends\" : \"amount of money spent on clicks generated by product based search in the week (Non - Branded keywords) \",\n",
    "            \"brand_level_branded_clicks\" : \"clicks generated by brand based search in the week (Branded keywords) \",\n",
    "            \"brand_level_branded_spends\" : \"amount of money spent on clicks generated by product based search in the week (Non - Branded keywords) \",\n",
    "            \"brand_level_nonbranded_clicks\" : \"clicks generated by brand based search in the week (Branded keywords) \",\n",
    "            \"brand_level_nonbranded_spends\" : \"amount of money spent on clicks generated by product based search in the week (Non - Branded keywords) \",\n",
    "            \"insta_clicks\" : \"clicks generated on instagram advertizement in the week.\",\n",
    "            \"insta_spends\" : \"amount of money spent on clicks generated on instagram advertizement in the week.\",\n",
    "            \"fb_clicks\" : \"clicks generated on facebook advertizement in the week.\",\n",
    "            \"fb_spends\" : \"amount of money spent on clicks generated on facebook advertizement in the week.\"\n",
    "                }\n",
    "    }\n",
    "    )\n",
    "\n",
    "profile.to_file(Path(\"stata_auto_report.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file('report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = profile.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"your_report.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def load_dataframe(df):\n",
    "    \"\"\"Load the dataframe in the context\"\"\"\n",
    "    return configuration['master_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def QuickChartTool(chart_config):\n",
    "    \"\"\"Generates a chart image URL from the provided Chart.js configuration.\"\"\"\n",
    "    chart_config_str = str(chart_config)  # ensure input is string\n",
    "    url = f\"https://quickchart.io/chart?c={quote(chart_config_str)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time vs clicks\n",
    "# spends table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def compute_summary_statistics(data : pd.DataFrame, columns: list) -> str:\n",
    "    \"\"\"Compute summary statistics for specified columns in the dataset.\"\"\"\n",
    "    stats = data[columns].describe().to_dict()\n",
    "    return str(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool = llm.bind_tools([compute_summary_statistics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel):\n",
    "    purpose : str = Field(description=\"Summarize key findings from the raw data review to ensure data readiness for Market Mix Modeling, for the given data\")\n",
    "    observations : str = Field(description=\"\"\"\n",
    "                               Overview of data quality, completeness, and potential issues.\n",
    "                               High-level insights on correlations, trends, outliers, and anomalies.\n",
    "                               Recommendations for data preparation and treatment, for the given data.\"\"\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    data_sources : str = Field(description=\"\"\"\n",
    "                               List of data sources (e.g., sales data, media spend, macroeconomic indicators).\n",
    "                               Time period covered (e.g., January 2020 - December 2024).\n",
    "                               Granularity (e.g., weekly, monthly, regional).\"\"\")\n",
    "    observations : str = Field(description=\"\"\"\n",
    "                               **Target Variable**: Description of the target KPI (e.g., sales revenue, units sold).\n",
    "                               **Independent Variables**: Media spends (TV, digital, print), promotions, pricing, macroeconomic factors, etc.\"\"\")\n",
    "    \n",
    "    data_volume : str = Field(description=\"Number of observations, variables, and records per product/region.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveStats(BaseModel):\n",
    "    summary_stats : str = Field(description=\"\"\"\n",
    "                                Mean, median, standard deviation, min, max for key variables (e.g., sales, media spends).\n",
    "                                Distribution characteristics (e.g., skewness, kurtosis).\"\"\")\n",
    "    market_share_analysis : str = Field(description=\"\"\"\n",
    "                                Market share by product/brand over time (percentage of total sales).\n",
    "                                Visual: Pie chart or stacked bar chart showing market share distribution.\"\"\")\n",
    "    \n",
    "    spend_distribution : str = Field(description=\"\"\"\n",
    "                                Breakdown of media and promotional spends by channel (e.g., TV, digital, radio) in tabular form.\n",
    "                                Visual: Bar chart or histogram of spend distribution across channels.\n",
    "                                Insights: Identify dominant channels and variability in spend patterns.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    data : pd.DataFrame\n",
    "    combined_summary : str\n",
    "    combined_overview : str\n",
    "    combined_stats : str\n",
    "    combined : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(state: State):\n",
    "    summary_report =  llm_with_tool.with_structured_output(Summary).invoke(f\"use appropriate tool to write a detailed summary for the data : data : {state['data']}\")\n",
    "    purpose = summary_report.purpose\n",
    "    observations = summary_report.observations\n",
    "    combined_summary = f\"\"\"\n",
    "## purpose \n",
    "{purpose} \n",
    "## obeservations \n",
    "{observations}\n",
    "                    \"\"\"\n",
    "    return {'combined_summary' : combined_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_overview(state: State):\n",
    "    overview_report =  llm_with_tool.with_structured_output(Overview).invoke(f\"{state['data']}\")\n",
    "    data_sources = overview_report.data_sources\n",
    "    observations = overview_report.observations\n",
    "    data_volume = overview_report.data_volume\n",
    "    combined_overview = f\"\"\"\n",
    "## data_sources \n",
    "{data_sources} \n",
    "## observations \n",
    "{observations}\n",
    "## data_volume \n",
    "{data_volume}\n",
    "                    \"\"\"\n",
    "    return {'combined_overview' : combined_overview}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_descriptive_stats(state: State):\n",
    "    stats_report =  llm_with_tool.with_structured_output(DescriptiveStats).invoke(f\"{state['data']}\")\n",
    "    summary_stats = stats_report.summary_stats\n",
    "    market_share_analysis = stats_report.market_share_analysis\n",
    "    spend_distribution = stats_report.spend_distribution\n",
    "    combined_stats = f\"\"\"\n",
    "## summary_stats \n",
    "{summary_stats} \n",
    "## market_share_analysis \n",
    "{market_share_analysis}\n",
    "## spend_distribution \n",
    "{spend_distribution}\n",
    "                    \"\"\"\n",
    "    return {'combined_stats' : combined_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(state: State):\n",
    "    \"\"\" Combine every thing into a single format\"\"\"\n",
    "    combined_text = \"Here is the combined output : \\n\"\n",
    "    combined_text += f\"combined_summary: \\n {state['combined_summary']}\" \n",
    "    combined_text += f\"combined_overview: \\n {state['combined_overview']}\" \n",
    "    combined_text += f\"combined_stats: \\n {state['combined_stats']}\" \n",
    "    return {'combined': combined_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "parallel_builder.add_node(\"write_summary\",write_summary)\n",
    "parallel_builder.add_node(\"write_overview\",write_overview)\n",
    "parallel_builder.add_node(\"write_descriptive_stats\",write_descriptive_stats)\n",
    "parallel_builder.add_node(\"aggregator\",aggregator)\n",
    "\n",
    "\n",
    "parallel_builder.add_edge(START, \"write_summary\")\n",
    "parallel_builder.add_edge(START, \"write_overview\")\n",
    "parallel_builder.add_edge(START, \"write_descriptive_stats\")\n",
    "\n",
    "parallel_builder.add_edge(\"write_summary\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"write_overview\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"write_descriptive_stats\", \"aggregator\")\n",
    "\n",
    "parallel_builder.add_edge(\"aggregator\",END)\n",
    "\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        d = s[list(s.keys())[0]]\n",
    "        message = d[list(d.keys())[0]]\n",
    "        print(message)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = parallel_workflow.invoke({\"data\":configuration['master_data']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(output['combined_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stream(parallel_workflow.stream({\"data\":configuration['master_data']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in :\n",
    "    print(s)\n",
    "\n",
    "    # list(s.keys())[0]\n",
    "\n",
    "    d = s[list(s.keys())[0]]\n",
    "\n",
    "    print(d[list(d.keys())[0]])\n",
    "\n",
    "    \n",
    "    s[list(s.keys())[0]][list(s.keys())[0]]\n",
    "    import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stream(parallel_workflow.stream({\"data\":configuration['master_data']}))\n",
    "    # s[list(s.keys())[0]]\n",
    "# inputs = {\"messages\": [(\"user\", \"explain the data\")]}\n",
    "# print_stream(data_analysis_agent.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(output['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    name: str = Field(description=\"The title of the section of the report\")\n",
    "    description : str = Field(description=\"The detailed explaination of the observed or analysed number\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections : list[Section] = Field(description=\"list of all the sections\")\n",
    "\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    sections : List[Section]\n",
    "    completed_sections : Annotated[\n",
    "        List, operator.add\n",
    "    ]\n",
    "    final_report : str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    section : Section\n",
    "    completed_sections: Annotated[\n",
    "        List, operator.add\n",
    "        ]\n",
    "\n",
    "\n",
    "def orchestrator(state: State):\n",
    "    planning = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content = \"Plan the report into subsections based on the topic.\"),\n",
    "            HumanMessage(content = f\"Here is the topic : {state['topic']}\")\n",
    "        ]\n",
    "    )\n",
    "    return {'sections': planning.sections}\n",
    "\n",
    "def agents(state: AgentState):\n",
    "    contnt = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content = \"Generate a report section without preamble base on the given topic\"),\n",
    "            HumanMessage(content = f\"The name : {state['section'].name}, and the description is : {state['section'].description}\")\n",
    "        ]\n",
    "    )\n",
    "    return {\"completed_sections\": [contnt.content]}\n",
    "\n",
    "def agent_handler(state: State):\n",
    "    return [Send(\"agents\",{\"section\": s}) for s in state['sections']]\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    combined_sections = state['completed_sections']\n",
    "    final_report = \"\\n\\n --- \\n\\n\".join(combined_sections)\n",
    "\n",
    "    return {'final_report': final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_flow_builder = StateGraph(State)\n",
    "\n",
    "orchestrator_flow_builder.add_node(\"orchestrator\",orchestrator)\n",
    "orchestrator_flow_builder.add_node(\"agents\",agents)\n",
    "orchestrator_flow_builder.add_node(\"synthesizer\",synthesizer)\n",
    "\n",
    "orchestrator_flow_builder.add_edge(START, 'orchestrator')\n",
    "orchestrator_flow_builder.add_conditional_edges(\n",
    "    'orchestrator',\n",
    "    agent_handler,\n",
    "    [\"agents\"]\n",
    ")\n",
    "orchestrator_flow_builder.add_edge(\"agents\",\"synthesizer\")\n",
    "orchestrator_flow_builder.add_edge(\"synthesizer\",END)\n",
    "\n",
    "\n",
    "orchestrator_workflow = orchestrator_flow_builder.compile()\n",
    "\n",
    "display(Image(orchestrator_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a senior data analyst specializing in Marketing Mix Modeling (MMM).\n",
    "Your expertise includes:\n",
    "\n",
    "- Decomposing sales into base, incremental, and external drivers\n",
    "- ROI analysis of marketing levers (TV, digital, price cuts, etc.)\n",
    "- Trend, seasonality, and anomaly detection\n",
    "- Communicating insights clearly to business and marketing stakeholders\n",
    "- Visualizing insights using QuickChart (https://quickchart.io/)\n",
    "- Producing accurate reports in LaTeX format, suitable for PDF compilation\n",
    "\n",
    "\n",
    "## Report Format \n",
    "\n",
    "### Market Mix Modeling: Pre-Modeling Raw Data Review Report\n",
    "\n",
    "#### 1. Executive Summary\n",
    "- **Purpose**: Summarize key findings from the raw data review to ensure data readiness for Market Mix Modeling.\n",
    "- **Key Observations**:\n",
    "Overview of data quality, completeness, and potential issues.\n",
    "High-level insights on correlations, trends, outliers, and anomalies.\n",
    "Recommendations for data preparation and treatment.\n",
    "\n",
    "#### 2. Data Overview\n",
    "- **Data Sources**:\n",
    "List of data sources (e.g., sales data, media spend, macroeconomic indicators).\n",
    "Time period covered (e.g., January 2020 - December 2024).\n",
    "Granularity (e.g., weekly, monthly, regional).\n",
    "- **Key Variables**:\n",
    "**Target Variable**: Description of the target KPI (e.g., sales revenue, units sold).\n",
    "**Independent Variables**: Media spends (TV, digital, print), promotions, pricing, macroeconomic factors, etc.\n",
    "- **Data Volume**:\n",
    "Number of observations, variables, and records per product/region.\n",
    "\n",
    "#### 3. Descriptive Statistics\n",
    "- **Summary Statistics**:\n",
    "Mean, median, standard deviation, min, max for key variables (e.g., sales, media spends).\n",
    "Distribution characteristics (e.g., skewness, kurtosis).\n",
    "- **Market Share Analysis**:\n",
    "Market share by product/brand over time (percentage of total sales).\n",
    "Visual: Pie chart or stacked bar chart showing market share distribution.\n",
    "- **Spend Distribution**:\n",
    "Breakdown of media and promotional spends by channel (e.g., TV, digital, radio).\n",
    "Visual: Bar chart or histogram of spend distribution across channels.\n",
    "Insights: Identify dominant channels and variability in spend patterns.\n",
    "\n",
    "#### 4. Correlation Analysis\n",
    "- **Inter-KPI Correlation**:\n",
    "Correlation matrix of independent variables (e.g., TV spend vs. digital spend).\n",
    "Visual: Heatmap of correlations to identify multicollinearity risks.\n",
    "Insights: Highlight high correlations (>0.7) that may require variable reduction or transformation.\n",
    "- **Correlation with Target**:\n",
    "Correlation coefficients between each independent variable and the target (e.g., sales).\n",
    "Visual: Bar chart ranking variables by correlation strength.\n",
    "Insights: Identify key drivers and weak predictors.\n",
    "\n",
    "#### 5. Trend Analysis\n",
    "- **Time Series Trends**:\n",
    "Trends for target variable and key independent variables over time.\n",
    "Visual: Line charts showing trends for sales, media spends, and promotions.\n",
    "Insights: Identify seasonality, cyclical patterns, or structural breaks.\n",
    "- **Product-Level Trends**:\n",
    "Trends in sales and market share by product.\n",
    "Visual: Stacked line charts or area charts for product-level trends.\n",
    "\n",
    "#### 6. Outlier Identification\n",
    "- **Methodology**:\n",
    "Statistical methods used (e.g., Z-score, IQR, or domain-specific thresholds).\n",
    "Example: Z-score > 3 or < -3 for sales or spend data.\n",
    "- **Findings**:\n",
    "List of outliers by variable (e.g., unusually high TV spend in Q3 2023).\n",
    "Visual: Box plots or scatter plots highlighting outliers.\n",
    "- **Impact Assessment**:\n",
    "Potential impact of outliers on model performance.\n",
    "Recommendations: Winsorization, capping, or removal.\n",
    "\n",
    "#### 7. Missing Data Analysis\n",
    "- **Missing Data Summary**:\n",
    "Percentage of missing values by variable and time period.\n",
    "Visual: Heatmap or bar chart of missing data patterns.\n",
    "- **Patterns**:\n",
    "Random vs. systematic missingness (e.g., missing digital spend data for specific regions).\n",
    "- **Recommendations**:\n",
    "Imputation methods (e.g., mean/median imputation, time-series interpolation).\n",
    "Exclusion of variables with excessive missingness (>30%).\n",
    "\n",
    "#### 8. Anomaly Detection\n",
    "- **Methodology**:\n",
    "Techniques used (e.g., statistical thresholds, clustering, or machine learning-based anomaly detection).\n",
    "- **Findings**:\n",
    "Specific anomalies (e.g., sudden sales spike in a region unrelated to marketing activity).\n",
    "Visual: Time-series plots with flagged anomalies.\n",
    "- **Potential Causes**:\n",
    "Data entry errors, external events, or untracked campaigns.\n",
    "- **Recommendations**:\n",
    "Investigate anomalies with stakeholders.\n",
    "Adjust data or include dummy variables for known events.\n",
    "\n",
    "#### 9. Data Quality Issues and Treatment Suggestions\n",
    "- **Summary of Issues**:\n",
    "Multicollinearity, outliers, missing data, anomalies, or inconsistent granularity.\n",
    "- **Proposed Treatments**:\n",
    "**Multicollinearity**: Combine correlated variables or use PCA.\n",
    "**Outliers**: Winsorize, cap, or remove based on domain knowledge.\n",
    "**Missing Data**: Impute using time-series methods or exclude problematic variables.\n",
    "**Anomalies**: Add dummy variables for known events or remove erroneous records.\n",
    "**Normalization**: Scale variables (e.g., log transformation for skewed spends).\n",
    "- **Next Steps**:\n",
    "Validate treatments with stakeholders.\n",
    "Prepare cleaned dataset for modeling.\n",
    "\n",
    "#### 10. Appendix\n",
    "- **Data Dictionary**:\n",
    "Definitions and sources for all variables.\n",
    "- **Additional Visualizations**:\n",
    "Detailed charts or tables (e.g., raw data samples, additional trend plots).\n",
    "- **Technical Notes**:\n",
    "Software/tools used (e.g., Python, R, Excel for analysis).\n",
    "Code snippets for key analyses (e.g., correlation matrix, outlier detection).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Data Exploration Phase\n",
    "   - Load dataset using `load_dataframe`\n",
    "   - Identify the dependent variable (e.g., sales)\n",
    "   - Categorize independent variables as base, incremental, or external\n",
    "\n",
    "2. Visualization Phase (Use QuickChart)\n",
    "   - Generate chart URLs using the `QuickChartTool` by passing Chart.js JSON configs.\n",
    "   - Embed these charts in LaTeX using the following syntax:\n",
    "     \\\\\n",
    "     \\\\begin{figure}[H]\n",
    "     \\\\centering\n",
    "     \\\\includegraphics[width=\\\\linewidth]{{<chart_url>}}\n",
    "     \\\\caption{<caption_text>}\n",
    "     \\\\end{figure}\n",
    "   - Use these charts where relevant:\n",
    "     - Line chart for sales trends over time\n",
    "     - Bar chart for driver contributions\n",
    "     - Pie chart for driver share breakdown\n",
    "     - Heatmap-like correlation matrix (mock with bar/stacked bar if needed)\n",
    "     - Scatter charts for bivariate analysis (optional with annotations)\n",
    "     - Bar chart of missing values by column\n",
    "\n",
    "3. Insight Generation Phase\n",
    "   - Explain all chart outputs clearly in LaTeX text\n",
    "   - Identify missing values, correlations, lag/adstock effects\n",
    "   - Quantify driver impact\n",
    "   - Highlight anomalies (peaks, dips, or seasonal trends)\n",
    "   - Recommend treatment for missing data and campaign gaps\n",
    "\n",
    "4. Report Writing Phase\n",
    "   - Use LaTeX sectioning commands (\\\\section, \\\\subsection, etc.) to structure the report\n",
    "   - Embed charts as figures with captions as shown above\n",
    "   - Avoid hallucination or assumption — base all insights on real data\n",
    "   - Provide concise bullet points using LaTeX itemize environment\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "\\\\section*{Executive Summary}\n",
    "- Key sales trends\n",
    "- Major drivers (TV, price, etc.)\n",
    "- Seasonal effects or spikes\n",
    "- Anomalies or potential data issues\n",
    "\n",
    "\\\\section*{Variable Classification}\n",
    "- Table or list of variables: base, incremental, external with reasoning\n",
    "\n",
    "\\\\section*{Data Summary}\n",
    "- Describe each column and its role\n",
    "- Mention missing data:\n",
    "  - Count & % per column\n",
    "  - Likely causes (e.g., campaign gaps, bad logging)\n",
    "  - Recommended treatment (e.g., fill with 0, interpolate, drop)\n",
    "\n",
    "\\\\section*{Univariate Analysis}\n",
    "- Distribution patterns for each variable\n",
    "- Include bar or pie charts with QuickChart embedded as figures\n",
    "\n",
    "\\\\section*{Bivariate & Correlation Analysis}\n",
    "- Explain relationships between drivers and sales\n",
    "- Describe strength, direction, and potential multicollinearity\n",
    "\n",
    "\\\\section*{Adstock & Lag Effects}\n",
    "- Describe persistence/memory effects of media\n",
    "- Explain where diminishing returns or delay patterns exist\n",
    "\n",
    "\\\\section*{Outlier & Peak Analysis}\n",
    "- Describe any anomalies\n",
    "- Hypothesize missed events, overspend, or missing data\n",
    "\n",
    "\\\\section*{Business Recommendations}\n",
    "- Spend allocation advice (e.g., increase digital in Q2)\n",
    "- Suggestions for data quality improvement\n",
    "- Notes for modeling enhancements (e.g., separate halo/cannibal effects)\n",
    "\n",
    "---\n",
    "Analysis conducted by: MMM Agent \\\\\\\\\n",
    "Report generated on: {current_date}\n",
    "\n",
    "Use the following tools as needed: {tools}\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "Action: {action}\n",
    "Action Input: {action_input}\n",
    "Observation: {observation}\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: {final_answer}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "context : {configuration['master_data']}\n",
    "\n",
    "## Market Mix Modeling: Pre-Modeling Raw Data Review Report\n",
    "\n",
    "#### 1. Executive Summary\n",
    "- **Purpose**: Summarize key findings from the raw data review to ensure data readiness for Market Mix Modeling.\n",
    "- **Key Observations**:\n",
    "Overview of data quality, completeness, and potential issues.\n",
    "High-level insights on correlations, trends, outliers, and anomalies.\n",
    "Recommendations for data preparation and treatment.\n",
    "\n",
    "#### 2. Data Overview\n",
    "- **Data Sources**:\n",
    "List of data sources (e.g., sales data, media spend, macroeconomic indicators).\n",
    "Time period covered (e.g., January 2020 - December 2024).\n",
    "Granularity (e.g., weekly, monthly, regional).\n",
    "- **Key Variables**:\n",
    "**Target Variable**: Description of the target KPI (e.g., sales revenue, units sold).\n",
    "**Independent Variables**: Media spends (TV, digital, print), promotions, pricing, macroeconomic factors, etc.\n",
    "- **Data Volume**:\n",
    "Number of observations, variables, and records per product/region.\n",
    "\n",
    "#### 3. Descriptive Statistics\n",
    "- **Summary Statistics**:\n",
    "Mean, median, standard deviation, min, max for key variables (e.g., sales, media spends).\n",
    "Distribution characteristics (e.g., skewness, kurtosis).\n",
    "- **Market Share Analysis**:\n",
    "Market share by product/brand over time (percentage of total sales).\n",
    "Visual: Pie chart or stacked bar chart showing market share distribution.\n",
    "- **Spend Distribution**:\n",
    "Breakdown of media and promotional spends by channel (e.g., TV, digital, radio).\n",
    "Visual: Bar chart or histogram of spend distribution across channels.\n",
    "Insights: Identify dominant channels and variability in spend patterns.\n",
    "\n",
    "#### 4. Correlation Analysis\n",
    "- **Inter-KPI Correlation**:\n",
    "Correlation matrix of independent variables (e.g., TV spend vs. digital spend).\n",
    "Visual: Heatmap of correlations to identify multicollinearity risks.\n",
    "Insights: Highlight high correlations (>0.7) that may require variable reduction or transformation.\n",
    "- **Correlation with Target**:\n",
    "Correlation coefficients between each independent variable and the target (e.g., sales).\n",
    "Visual: Bar chart ranking variables by correlation strength.\n",
    "Insights: Identify key drivers and weak predictors.\n",
    "\n",
    "#### 5. Trend Analysis\n",
    "- **Time Series Trends**:\n",
    "Trends for target variable and key independent variables over time.\n",
    "Visual: Line charts showing trends for sales, media spends, and promotions.\n",
    "Insights: Identify seasonality, cyclical patterns, or structural breaks.\n",
    "- **Product-Level Trends**:\n",
    "Trends in sales and market share by product.\n",
    "Visual: Stacked line charts or area charts for product-level trends.\n",
    "\n",
    "#### 6. Outlier Identification\n",
    "- **Methodology**:\n",
    "Statistical methods used (e.g., Z-score, IQR, or domain-specific thresholds).\n",
    "Example: Z-score > 3 or < -3 for sales or spend data.\n",
    "- **Findings**:\n",
    "List of outliers by variable (e.g., unusually high TV spend in Q3 2023).\n",
    "Visual: Box plots or scatter plots highlighting outliers.\n",
    "- **Impact Assessment**:\n",
    "Potential impact of outliers on model performance.\n",
    "Recommendations: Winsorization, capping, or removal.\n",
    "\n",
    "#### 7. Missing Data Analysis\n",
    "- **Missing Data Summary**:\n",
    "Percentage of missing values by variable and time period.\n",
    "Visual: Heatmap or bar chart of missing data patterns.\n",
    "- **Patterns**:\n",
    "Random vs. systematic missingness (e.g., missing digital spend data for specific regions).\n",
    "- **Recommendations**:\n",
    "Imputation methods (e.g., mean/median imputation, time-series interpolation).\n",
    "Exclusion of variables with excessive missingness (>30%).\n",
    "\n",
    "#### 8. Anomaly Detection\n",
    "- **Methodology**:\n",
    "Techniques used (e.g., statistical thresholds, clustering, or machine learning-based anomaly detection).\n",
    "- **Findings**:\n",
    "Specific anomalies (e.g., sudden sales spike in a region unrelated to marketing activity).\n",
    "Visual: Time-series plots with flagged anomalies.\n",
    "- **Potential Causes**:\n",
    "Data entry errors, external events, or untracked campaigns.\n",
    "- **Recommendations**:\n",
    "Investigate anomalies with stakeholders.\n",
    "Adjust data or include dummy variables for known events.\n",
    "\n",
    "#### 9. Data Quality Issues and Treatment Suggestions\n",
    "- **Summary of Issues**:\n",
    "Multicollinearity, outliers, missing data, anomalies, or inconsistent granularity.\n",
    "- **Proposed Treatments**:\n",
    "**Multicollinearity**: Combine correlated variables or use PCA.\n",
    "**Outliers**: Winsorize, cap, or remove based on domain knowledge.\n",
    "**Missing Data**: Impute using time-series methods or exclude problematic variables.\n",
    "**Anomalies**: Add dummy variables for known events or remove erroneous records.\n",
    "**Normalization**: Scale variables (e.g., log transformation for skewed spends).\n",
    "- **Next Steps**:\n",
    "Validate treatments with stakeholders.\n",
    "Prepare cleaned dataset for modeling.\n",
    "\n",
    "#### 10. Appendix\n",
    "- **Data Dictionary**:\n",
    "Definitions and sources for all variables.\n",
    "- **Additional Visualizations**:\n",
    "Detailed charts or tables (e.g., raw data samples, additional trend plots).\n",
    "- **Technical Notes**:\n",
    "Software/tools used (e.g., Python, R, Excel for analysis).\n",
    "Code snippets for key analyses (e.g., correlation matrix, outlier detection).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = orchestrator_workflow.invoke({\"topic\": prompt})\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Markdown(state['final_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_agent = create_react_agent(\n",
    "    model = llm,\n",
    "    tools = [load_dataframe, QuickChartTool],\n",
    "    prompt = prompt,\n",
    "    name = \"data_analysis_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "# task\n",
    "# context\n",
    "# example\n",
    "# persona\n",
    "# format\n",
    "# tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"explain the data\")]}\n",
    "# print_stream(data_analysis_agent.stream(inputs, stream_mode=\"values\"))\n",
    "data_analysis_agent.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_weeks = 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: \n",
    "# 1. RAG load - https://www.latentview.com/wp-content/uploads/2019/08/Marketing-Mix-Model.pdf\n",
    "# 2. Generate prompt\n",
    "# 3. react agent to dataAnalysis\n",
    "# 4. crewai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import RagTool\n",
    "\n",
    "# Create a RAG tool with default settings\n",
    "rag_tool = RagTool()\n",
    "\n",
    "# Add content from a file\n",
    "rag_tool.add(data_type=\"file\", path=\"path/to/your/document.pdf\")\n",
    "\n",
    "# Add content from a web page\n",
    "# rag_tool.add(data_type=\"web_page\", url=\"https://example.com\")\n",
    "# https://develop.nielsen.com/wp-content/uploads/sites/2/2019/04/marketing-mix-modeling-what-marketers-need-to-know.pdf\n",
    "\n",
    "# Define an agent with the RagTool\n",
    "@agent\n",
    "def knowledge_expert(self) -> Agent:\n",
    "    '''\n",
    "    This agent uses the RagTool to answer questions about the knowledge base.\n",
    "    '''\n",
    "    return Agent(\n",
    "        config=self.agents_config[\"knowledge_expert\"],\n",
    "        allow_delegation=False,\n",
    "        tools=[rag_tool]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
